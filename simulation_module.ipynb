{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from utils import get_interaction_network\n",
    "import yaml\n",
    "import random\n",
    "from munch import munchify\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    doc = yaml.safe_load(f)\n",
    "config = munchify(doc)\n",
    "\n",
    "#%% READ CONSTANTS FROM CONFIG\n",
    "N = config.params.N\n",
    "runs = config.params.runs\n",
    "convergence_time = config.params.convergence_time\n",
    "rewards_set = config.params.rewards_set\n",
    "memory_size_set = config.params.memory_size_set\n",
    "initial_composition = config.params.initial_composition\n",
    "initial = config.params.initial\n",
    "total_interactions = config.params.total_interactions\n",
    "temperature = config.params.temperature\n",
    "committment_index = config.minority.committment_index\n",
    "convergence_threshold = config.params.convergence_threshold\n",
    "stochastic = config.sim.stochastic\n",
    "\n",
    "options_set = config.params.options_set\n",
    "minority_size_set = config.minority.minority_size_set\n",
    "network_type = config.network.network_type\n",
    "version = config.sim.version\n",
    "initial = config.params.initial\n",
    "initial_composition = config.params.initial_composition\n",
    "continue_evolution = config.sim.continue_evolution\n",
    "\n",
    "if temperature == 0:\n",
    "    llm_params = {\"do_sample\": False,\n",
    "            \"max_new_tokens\": 6,\n",
    "            \"return_full_text\": False, \n",
    "            }\n",
    "else:\n",
    "    llm_params = {\"do_sample\": True,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_k\": 10,\n",
    "            \"max_new_tokens\": 6,\n",
    "            \"return_full_text\": False, \n",
    "            }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = ''   \n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(payload):\n",
    "    \"Query the Hugging Face API\"\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=payload).json()\n",
    "    except:\n",
    "        return None\n",
    "    return response\n",
    "\n",
    "def get_response(chat, options):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "\n",
    "    overloaded = 1\n",
    "    while overloaded == 1:\n",
    "        response = query({\"inputs\": chat, \"parameters\": llm_params, \"options\": {\"use_cache\": False}})\n",
    "        #print(response)\n",
    "        if response == None:\n",
    "            print('CAUGHT JSON ERROR')\n",
    "            continue\n",
    "\n",
    "        if type(response)==dict:\n",
    "            print(\"AN EXCEPTION: \", response)\n",
    "            time.sleep(2.5)\n",
    "            if \"Inference Endpoints\" in response['error']:\n",
    "              print(\"HOURLY RATE LIMIT REACHED\")\n",
    "              time.sleep(450)\n",
    "                \n",
    "        elif any(option in response[0]['generated_text'].split(\"'\") for option in options):\n",
    "            overloaded=0\n",
    "    response_split = response[0]['generated_text'].split(\"'\")\n",
    "    for opt in options:\n",
    "        try:\n",
    "            index = response_split.index(opt)\n",
    "        except:\n",
    "            continue\n",
    "    #print(response_split[index])\n",
    "    return response_split[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(rewards, options):\n",
    "    incorrect, correct = rewards\n",
    "\n",
    "    rule_set = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    Context: Player 1 is playing a multi-round partnership game with Player 2 for 100 rounds.\n",
    "    At each round, Player 1 and Player 2 simultaneously pick an action from the following values: {options}.\n",
    "    The payoff that both players get is determined by the following rule:\n",
    "    1. If Players play the SAME action as each other, they will both be REWARDED with payoff {correct} points.\n",
    "    2. If Players play DIFFERENT actions to each other, they will both be PUNISHED with payoff {incorrect} points. \n",
    "    The objective of each Player is to maximize their own accumulated point tally, conditional on the behavior of the other player.\n",
    "    \"\"\" \n",
    "    return rule_set\n",
    "def get_outcome(my_answer, partner_answer, rewards):\n",
    "    if my_answer == partner_answer:\n",
    "        return rewards[1]\n",
    "    return rewards[0]\n",
    "\n",
    "\n",
    "def get_prompt(player, memory_size, rules):\n",
    "\n",
    "  # add initial round\n",
    "  new_query = f\"It is now round 1.\" + \" The current score of Player 1 is 0. Answer saying which value Player 1 should pick. Please think step by step before making a decision. Remember, examining history explicitly is important. Write your answer using the following format: {'value': <VALUE_OF_PLAYER_1>; 'reason': <YOUR_REASON>}. <|eot_id|><|start_header_id|>user<|end_header_id|> Answer saying which action Player 1 should play. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "  l = len(player['my_history'])\n",
    "  if l == 0:\n",
    "    return \"\"\"\\n \"\"\".join([rules, new_query])\n",
    "  \n",
    "  current_score = 0 #local score tracking --ignores global scoring.\n",
    "  history_intro = \"This is the history of choices in past rounds:\"\n",
    "  histories = []\n",
    "  if l < memory_size:\n",
    "    for idx in range(l):\n",
    "      my_answer = player['my_history'][idx] \n",
    "      partner_answer = player['partner_history'][idx] \n",
    "      outcome = player['outcome'][idx]\n",
    "      current_score+=outcome\n",
    "      histories.append({'round':idx+1, 'Player 1':my_answer, 'Player 2':partner_answer, 'payoff':outcome})\n",
    "  \n",
    "  if l >= memory_size:\n",
    "    indices = list(range(l))[-memory_size:]\n",
    "    for idx, r in enumerate(indices):\n",
    "      my_answer = player['my_history'][r] \n",
    "      partner_answer = player['partner_history'][r] \n",
    "      outcome = player['outcome'][r] \n",
    "      current_score+=outcome\n",
    "      histories.append({'round':idx+1, 'Player 1':my_answer, 'Player 2':partner_answer, 'payoff':outcome})\n",
    "  \n",
    "  new_query = f\"It is now round {idx+2}. The current score of Player 1 is {current_score}.\" + \" Answer saying which value Player 1 should pick. Please think step by step before making a decision. Remember, examining history explicitly is important. Write your answer using the following format: {'value': <VALUE_OF_PLAYER_1>; 'reason': <YOUR_REASON>}. <|eot_id|><|start_header_id|>user<|end_header_id|> Answer saying which action Player 1 should play. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "  histories = \"\\n \".join([f\"{hist}\" for hist in histories])\n",
    "  prompt = \"\"\"\\n \"\"\".join([rules, history_intro, histories, new_query])\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(player, my_answer, partner_answer, outcome):\n",
    "  player['score'] += outcome\n",
    "  player['my_history'].append(my_answer)\n",
    "  player['partner_history'].append(partner_answer)\n",
    "  player['score_history'].append(player['score'])\n",
    "  player['outcome'].append(outcome)\n",
    "\n",
    "  return player\n",
    "\n",
    "def has_tracker_converged(tracker, threshold = convergence_threshold):\n",
    "    if sum(tracker['outcome'][-convergence_time:]) < threshold*convergence_time:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def update_tracker(tracker, p1, p2, p1_answer, p2_answer, outcome):\n",
    "  tracker['players'].append([p1, p2])\n",
    "  tracker['answers'].append([p1_answer, p2_answer])\n",
    "  if outcome > 5:\n",
    "    tracker['outcome'].append(1)\n",
    "  else:\n",
    "    tracker['outcome'].append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converge(dataframe, run, memory_size, rewards, options, fname):\n",
    "    new_options = options.copy()\n",
    "    interaction_dict = dataframe['simulation']\n",
    "    tracker = dataframe['tracker']\n",
    "    if stochastic == True:\n",
    "        while has_tracker_converged(tracker) == False:\n",
    "            # randomly choose player and a neighbour\n",
    "            p1 = random.choice(list(interaction_dict.keys()))\n",
    "            p2 = random.choice(interaction_dict[p1]['neighbours'])\n",
    "            \n",
    "            # add interactions to play history\n",
    "            \n",
    "            interaction_dict[p1]['interactions'].append(p2)\n",
    "            interaction_dict[p2]['interactions'].append(p1)\n",
    "            p1_dict = interaction_dict[p1]\n",
    "            p2_dict = interaction_dict[p2]\n",
    "            \n",
    "            # play\n",
    "\n",
    "            answers = []\n",
    "            for player in [p1_dict, p2_dict]:\n",
    "                random.shuffle(new_options)\n",
    "                rules = get_rules(rewards, options = new_options)\n",
    "                # get prompt with rules & history of play\n",
    "                prompt = get_prompt(player, memory_size=memory_size, rules = rules)\n",
    "\n",
    "                # get agent response\n",
    "                answers.append(get_response(prompt, options=new_options))\n",
    "                    \n",
    "            my_answer, partner_answer = answers\n",
    "\n",
    "            # calculate outcome and update dictionary\n",
    "            \n",
    "            outcome = get_outcome(my_answer, partner_answer, rewards)\n",
    "            interaction_dict[p1] = update_dict(p1_dict, my_answer, partner_answer, outcome)\n",
    "            interaction_dict[p2] = update_dict(p2_dict, partner_answer, my_answer, outcome)\n",
    "            update_tracker(tracker, p1, p2, my_answer, partner_answer, outcome)\n",
    "            \n",
    "            if len(tracker['outcome']) % 50 == 0:\n",
    "                print(f\"STOCHASTIC RUN {run} -- INTERACTION {len(tracker['outcome'])}\")\n",
    "                dataframe['simulation'] = interaction_dict\n",
    "                dataframe['tracker'] = tracker\n",
    "                f = open(fname, 'wb')\n",
    "                pickle.dump(dataframe, f)\n",
    "                f.close()\n",
    "\n",
    "    if stochastic == False:\n",
    "        player_ids = list(range(1,N+1))\n",
    "        # Run the simulation for N*total_interaction interactions.\n",
    "        # Note: here 'total_interactions' is used for the number of population rounds in the NG.\n",
    "        while len(tracker['outcome']) < N*total_interactions:\n",
    "            random.shuffle(player_ids)\n",
    "            #random.shuffle(new_options)\n",
    "            #rules = get_rules(rewards, options = new_options)\n",
    "            pairs = [(player_ids[i], player_ids[i + 1]) for i in range(0, len(player_ids), 2)]\n",
    "            for pair in pairs:\n",
    "                random.shuffle(new_options)\n",
    "                rules = get_rules(rewards, options = new_options)\n",
    "                p1, p2 = pair\n",
    "                interaction_dict[p1]['interactions'].append(p2)\n",
    "                interaction_dict[p2]['interactions'].append(p1)\n",
    "                p1_dict = interaction_dict[p1]\n",
    "                p2_dict = interaction_dict[p2]\n",
    "                \n",
    "                # play\n",
    "\n",
    "                answers = []\n",
    "                for player in [p1_dict, p2_dict]:\n",
    "                    # get prompt with rules & history of play\n",
    "                    prompt = get_prompt(player, memory_size=memory_size, rules = rules)\n",
    "\n",
    "                    # get agent response\n",
    "                    answers.append(get_response(prompt, options=new_options))\n",
    "                        \n",
    "                my_answer, partner_answer = answers\n",
    "\n",
    "                # calculate outcome and update dictionary\n",
    "                \n",
    "                outcome = get_outcome(my_answer, partner_answer, rewards)\n",
    "                interaction_dict[p1] = update_dict(p1_dict, my_answer, partner_answer, outcome)\n",
    "                interaction_dict[p2] = update_dict(p2_dict, partner_answer, my_answer, outcome)\n",
    "                update_tracker(tracker, p1, p2, my_answer, partner_answer, outcome)\n",
    "                \n",
    "                if len(tracker['outcome']) % 50 == 0:\n",
    "                    print(f\"NON STOCHASTIC RUN {run} -- INTERACTION {len(tracker['outcome'])}\")\n",
    "                    dataframe['simulation'] = interaction_dict\n",
    "                    dataframe['tracker'] = tracker\n",
    "                    f = open(fname, 'wb')\n",
    "                    pickle.dump(dataframe, f)\n",
    "                    f.close()\n",
    "        \n",
    "\n",
    "    dataframe['simulation'] = interaction_dict\n",
    "    dataframe['tracker'] = tracker\n",
    "    dataframe['convergence'] = {'converged_index': len(tracker['outcome']), 'committed_to': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_CM(dataframe, run, memory_size, rewards, options, fname, total_interactions = total_interactions):\n",
    "    new_options = options.copy()\n",
    "    interaction_dict = dataframe['simulation']\n",
    "    tracker = dataframe['tracker']\n",
    "    init_tracker_len = dataframe['convergence']['converged_index']\n",
    "    while len(tracker['outcome']) - init_tracker_len < total_interactions:\n",
    "        random.shuffle(new_options)\n",
    "        rules = get_rules(rewards, options = new_options)\n",
    "\n",
    "        # randomly choose player and a neighbour\n",
    "        p1 = random.choice(list(interaction_dict.keys()))\n",
    "        p2 = random.choice(interaction_dict[p1]['neighbours'])\n",
    "        \n",
    "        # add interactions to play history\n",
    "        \n",
    "        interaction_dict[p1]['interactions'].append(p2)\n",
    "        interaction_dict[p2]['interactions'].append(p1)\n",
    "        p1_dict = interaction_dict[p1]\n",
    "        p2_dict = interaction_dict[p2]\n",
    "        \n",
    "        # play\n",
    "\n",
    "        answers = []\n",
    "        for player in [p1_dict, p2_dict]:\n",
    "            # check if committed. If True, play committed answer.\n",
    "            if player['committed_tag'] == True:\n",
    "                a = dataframe['convergence']['committed_to']\n",
    "                answers.append(a)\n",
    "            else:\n",
    "                # get prompt with rules & history of play\n",
    "                prompt = get_prompt(player, memory_size=memory_size, rules = rules)\n",
    "\n",
    "                # get agent response\n",
    "                answers.append(get_response(prompt, options=new_options))\n",
    "                \n",
    "        my_answer, partner_answer = answers\n",
    "\n",
    "        # calculate outcome and update dictionary\n",
    "        \n",
    "        outcome = get_outcome(my_answer, partner_answer, rewards)\n",
    "        interaction_dict[p1] = update_dict(p1_dict, my_answer, partner_answer, outcome)\n",
    "        interaction_dict[p2] = update_dict(p2_dict, partner_answer, my_answer, outcome)\n",
    "        update_tracker(tracker, p1, p2, my_answer, partner_answer, outcome)\n",
    "        \n",
    "        if len(tracker['outcome']) % 20 == 0:\n",
    "            print(fname)\n",
    "            print(f\"COMMITTED RUN {run} -- INTERACTION {len(tracker['outcome'])}\")\n",
    "            dataframe['simulation'] = interaction_dict\n",
    "            dataframe['tracker'] = tracker\n",
    "            f = open(fname, 'wb')\n",
    "            pickle.dump(dataframe, f)\n",
    "            f.close()\n",
    "\n",
    "\n",
    "    dataframe['simulation'] = interaction_dict\n",
    "    dataframe['tracker'] = tracker\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mainframe(fname):\n",
    "    try:\n",
    "        return pickle.load(open(fname, 'rb'))\n",
    "    except:\n",
    "        mainframe = dict()\n",
    "    print('CREATING EMPTY MAINFRAME')\n",
    "    return mainframe\n",
    "\n",
    "def set_initial_state(network_dict, rewards, options, memory_size):\n",
    "    if initial == 'None':\n",
    "        pass\n",
    "    if initial == 'random':\n",
    "            for m in range(memory_size):\n",
    "                for p in network_dict.keys():  \n",
    "                    try:\n",
    "                        a = network_dict[p]['committed_tag']\n",
    "                    except:\n",
    "                        a = False\n",
    "\n",
    "                    if a == False:\n",
    "                        my_choice = options[random.choice(range(len([0,1])))]\n",
    "                        partner_choice = options[random.choice(range(len([0,1])))]\n",
    "                        update_dict(network_dict[p], my_choice, partner_choice, get_outcome(my_answer=my_choice, partner_answer=partner_choice, rewards = rewards))\n",
    "\n",
    "\n",
    "    if type(initial) == int:\n",
    "        for m in range(memory_size):\n",
    "            for p in network_dict.keys():  \n",
    "                try:\n",
    "                    a = network_dict[p]['committed_tag']\n",
    "                except:\n",
    "                    a = False\n",
    "\n",
    "                if a == False:         \n",
    "                    update_dict(network_dict[p], options[initial], options[initial], rewards[1])\n",
    "\n",
    "def get_empty_dataframe(fname, minority_size = 0):\n",
    "    try:\n",
    "        dataframe = pickle.load(open(fname, 'rb'))\n",
    "    except:\n",
    "        dataframe = {'simulation': get_interaction_network(network_type = network_type, minority_size=0), 'tracker': {'players': [], 'answers': [], 'outcome': []}}\n",
    "    print(\"My history: \", dataframe['simulation'][1]['my_history'])\n",
    "    return dataframe\n",
    "\n",
    "def test_if_initialisation_worked(dataframe, memory_size, options):\n",
    "    counter = 0\n",
    "    for player in dataframe['simulation'].keys():\n",
    "        my_ans = dataframe['simulation'][player]['my_history'][:memory_size]\n",
    "        partner_ans = dataframe['simulation'][player]['partner_history'][:memory_size]\n",
    "        score = dataframe['simulation'][player]['outcome'][:memory_size]\n",
    "        if my_ans.count(options[initial]) == memory_size:\n",
    "            counter +=1\n",
    "    print(counter)\n",
    "    if counter == N:\n",
    "        return True\n",
    "    else:\n",
    "        raise ValueError(\"prepared initialisation failed\")\n",
    "    \n",
    "def get_prepared_dataframe(fname, rewards, options, minority_size, memory_size):\n",
    "    try:\n",
    "        return pickle.load(open(fname, 'rb'))\n",
    "    except:\n",
    "        dataframe = {'simulation': get_interaction_network(network_type = network_type, minority_size=minority_size), 'tracker': {'players': [], 'answers': [], 'outcome': []}}\n",
    "    print(\"---------- CREATING NEW INITIALISED DATAFRAME ----------\")\n",
    "    set_initial_state(dataframe['simulation'], rewards, options, memory_size)\n",
    "    dataframe['convergence'] = {'converged_index': 0, 'committed_to': options[1]}\n",
    "    \n",
    "    #print(dataframe[0]['simulation'][1]['my_history'])\n",
    "    test_if_initialisation_worked(dataframe, memory_size, options)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def get_random_initialisation(fname, rewards, options, minority_size, memory_size):\n",
    "    try:\n",
    "        return pickle.load(open(fname, 'rb'))\n",
    "    except:\n",
    "        dataframe = {'simulation': get_interaction_network(network_type = network_type, minority_size=minority_size), 'tracker': {'players': [], 'answers': [], 'outcome': []}}\n",
    "\n",
    "    set_initial_state(dataframe['simulation'], rewards, options, memory_size)\n",
    "    dataframe['convergence'] = {'converged_index': 0, 'committed_to': None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_committed(df, minority_size):\n",
    "    if minority_size > 0:  \n",
    "        committed_ids = random.sample(list(df['simulation'].keys()), k = minority_size)\n",
    "        for id in committed_ids:\n",
    "            df['simulation'][id]['committed_tag'] = True\n",
    "    return df\n",
    "    \n",
    "def add_committed(df, minority_size):\n",
    "    new_keys = [n+1 for n in range(N+minority_size)]\n",
    "    for n in new_keys:\n",
    "        nodes = [n+1 for n in range(N+minority_size)]\n",
    "        nodes.remove(n)\n",
    "        if n>N:\n",
    "            df['simulation'][n] = {'my_history': [], 'partner_history': [], 'interactions': [], 'score': 0, 'score_history': [], 'outcome': [], 'committed_tag': True, 'neighbours': []}\n",
    "        df['simulation'][n]['neighbours'] = nodes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rewards in rewards_set:\n",
    "    for memory_size in memory_size_set:\n",
    "        for cm in minority_size_set:\n",
    "            for options in options_set:\n",
    "                if cm == 0:\n",
    "                    print(\"\"\"\n",
    "                          ##########################\n",
    "                          ##########################\n",
    "                          ---- CONVERGENCE----------\n",
    "                          ##########################\n",
    "                          ##########################\n",
    "                          \"\"\")\n",
    "                    \n",
    "                    # first, we load a baseline model\n",
    "                    mainfname = '.pkl'\n",
    "                    # load a converged baseline\n",
    "                    if initial == 'None':\n",
    "                        mainfname = f\"llama31_70b_converged_baseline_{'_'.join(options)}_{rewards[0]}_{rewards[1]}_{memory_size}mem_{config.network.network_type}_{N}ps_{temperature}tmp.pkl\"\n",
    "                    \n",
    "                    else:\n",
    "                        mainfname = f\"llama31_70b_evolved_from_{initial}_{'_'.join(options)}_{rewards[0]}_{rewards[1]}_{memory_size}mem_{config.network.network_type}_{N}ps_{total_interactions}ints_{temperature}tmp.pkl\"\n",
    "                    print(mainfname)\n",
    "                    if config.sim.stochastic == False:\n",
    "                        print(\"---------- NON STOCHASTIC SIMULATION ----------\")\n",
    "                        mainfname = \"ns_\"+mainfname\n",
    "                        print(mainfname)\n",
    "                    mainframe = load_mainframe(mainfname)\n",
    "                    mainframe['rules'] = get_rules(rewards, options = options)\n",
    "\n",
    "                    # run until sim converges\n",
    "                    for run in range(runs):\n",
    "                        \n",
    "                        \n",
    "                        temp_fname = \"temporary_\" + mainfname\n",
    "                        #df = load_mainframe(fname = temp_fname)\n",
    "                        if initial == 'None':\n",
    "                            if len(mainframe.keys())-1 > run:\n",
    "                                continue\n",
    "                            print(\"---------- BASELINE CONVERGENCE ----------\")\n",
    "                            df = get_empty_dataframe(fname=temp_fname)\n",
    "                            converge(dataframe=df, run=run, memory_size=memory_size, rewards=rewards, options=options, fname=temp_fname)\n",
    "                        if initial != 'None':\n",
    "                            if len(mainframe.keys())-1 > run:\n",
    "                                continue\n",
    "                            df = get_prepared_dataframe(fname=temp_fname, rewards=rewards, options=options, minority_size=cm, memory_size=memory_size)\n",
    "                            print(\"---------- CONTINUING EVOLUTION ----------\")\n",
    "                            print(f\"--- STARTING RUN {run} ---\")\n",
    "                            simulate_CM(dataframe=df, run=run, memory_size=memory_size, rewards=rewards, options=options, fname=temp_fname, total_interactions=total_interactions)\n",
    "                        print(run)\n",
    "                        # save in main dataframe\n",
    "                        mainframe[run] = df\n",
    "\n",
    "                        f = open(mainfname, 'wb')\n",
    "                        pickle.dump(mainframe, f)\n",
    "                        f.close()\n",
    "\n",
    "                        # delete temporary file\n",
    "                        file_to_rem = Path(temp_fname)\n",
    "                        file_to_rem.unlink(missing_ok=True)\n",
    "\n",
    "for rewards in rewards_set:\n",
    "    for memory_size in memory_size_set:\n",
    "        for run in range(runs):\n",
    "            for cm in minority_size_set:\n",
    "                for options in options_set:\n",
    "                    if cm > 0:\n",
    "                        if initial != 'None':\n",
    "                            mainframe = get_prepared_dataframe(fname='.pkl', rewards=rewards, options=options, minority_size=0, memory_size=memory_size)\n",
    "                        else:\n",
    "                            raise ValueError(\"baseline does not exist\")\n",
    "                            \n",
    "                        cmfname = f\"llama31_70b_{version}_{initial}_{cm}cmtd_{'_'.join(options)}_{rewards[0]}_{rewards[1]}_{memory_size}mem_{config.network.network_type}_{N}ps_{temperature}tmp.pkl\"\n",
    "                        print(cmfname)\n",
    "                        cmframe = load_mainframe(fname=cmfname)\n",
    "                        temp_fname = \"temporary_\" + cmfname\n",
    "                        print(\"cmframe keys:\", cmframe.keys())\n",
    "                        # check if we already simulated this run\n",
    "                        if len(cmframe.keys()) > run:\n",
    "                            df = cmframe[run]\n",
    "                        # if not, use old dataframe to run convergence.\n",
    "                        else:\n",
    "                            # load temporary dataframe\n",
    "\n",
    "                            df = load_mainframe(fname = temp_fname)\n",
    "\n",
    "                            # check if temporary dataframe is full.\n",
    "                            if len(df.keys()) == 0:\n",
    "                                print(f'----------STARTING RUN {run} FROM SCRATCH----------')\n",
    "                                df = mainframe\n",
    "                            \n",
    "                                # add committed agents to baseline dataframe\n",
    "                                if version == 'swap':\n",
    "                                    print(\"---------- SWAPPING COMMITTED AGENTS ----------\")\n",
    "                                    df = swap_committed(df, cm)\n",
    "                                \n",
    "                                if version == 'inject':\n",
    "                                    print(\"---------- ADDING COMMITTED AGENTS ----------\")\n",
    "                                    df = add_committed(df, cm)\n",
    "\n",
    "                            print(f\"Run: {run}\")\n",
    "                            print(f\"Initial population: {N}\")\n",
    "                            print(f\"There are {len(df['simulation'].keys())} players in the game\")\n",
    "                            print(f\"minority size: {cm}\")\n",
    "                            word =  df['convergence']['committed_to']\n",
    "                            print(f'committment word is: {word}')\n",
    "                            committed_agent_ids = [player for player in df['simulation'].keys() if df['simulation'][player]['committed_tag'] == True]\n",
    "                            print(f\"There are {len(committed_agent_ids)} committed agents: {committed_agent_ids}\")\n",
    "                            # run committed minorities\n",
    "                            print(\"---------- RUNNING COMMITTED AGENTS ----------\")\n",
    "                            simulate_CM(dataframe=df, run=run, memory_size=memory_size, rewards=rewards, options=options, fname=temp_fname, total_interactions=total_interactions)\n",
    "                            \n",
    "                            cmframe[run] = df\n",
    "                            # save in main dataframe\n",
    "                            f = open(cmfname, 'wb')\n",
    "                            pickle.dump(cmframe, f)\n",
    "                            f.close()\n",
    "            \n",
    "                            # delete temporary file\n",
    "                            file_to_rem = Path(temp_fname)\n",
    "                            file_to_rem.unlink(missing_ok=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
