{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Individual Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import utils\n",
    "import random\n",
    "import time\n",
    "import yaml\n",
    "from munch import munchify\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "def load_dataframe(fname):\n",
    "    try:\n",
    "        mainframe = pickle.load(open(fname, 'rb'))\n",
    "    except:\n",
    "        raise ValueError('NO DATAFILE FOUND')\n",
    "    \n",
    "    return mainframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    doc = yaml.safe_load(f)\n",
    "config = munchify(doc)\n",
    "\n",
    "#%% READ CONSTANTS FROM CONFIG\n",
    "N = config.params.N\n",
    "runs = config.params.runs\n",
    "convergence_time = config.params.convergence_time\n",
    "rewards_set = config.params.rewards_set\n",
    "memory_size_set = config.params.memory_size_set\n",
    "initial_composition = config.params.initial_composition\n",
    "initial = config.params.initial\n",
    "total_interactions = config.params.total_interactions\n",
    "temperature = config.params.temperature\n",
    "committment_index = config.minority.committment_index\n",
    "convergence_threshold = config.params.convergence_threshold\n",
    "stochastic = config.sim.stochastic\n",
    "\n",
    "options_set = config.params.options_set\n",
    "minority_size_set = config.minority.minority_size_set\n",
    "network_type = config.network.network_type\n",
    "version = config.sim.version\n",
    "initial = config.params.initial\n",
    "initial_composition = config.params.initial_composition\n",
    "continue_evolution = config.sim.continue_evolution\n",
    "\n",
    "if temperature == 0:\n",
    "    llm_params = {\"do_sample\": False,\n",
    "            \"max_new_tokens\": 6,\n",
    "            \"return_full_text\": False, \n",
    "            }\n",
    "else:\n",
    "    llm_params = {\"do_sample\": True,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_k\": 10,\n",
    "            \"max_new_tokens\": 6,\n",
    "            \"return_full_text\": False, \n",
    "            }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = ''   \n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(payload):\n",
    "    \"Query the Hugging Face API\"\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=payload).json()\n",
    "    except:\n",
    "        return None\n",
    "    return response\n",
    "\n",
    "def get_response(chat, options):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "\n",
    "    overloaded = 1\n",
    "    while overloaded == 1:\n",
    "        response = query({\"inputs\": chat, \"parameters\": llm_params, \"options\": {\"use_cache\": False}})\n",
    "        #print(response)\n",
    "        if response == None:\n",
    "            print('CAUGHT JSON ERROR')\n",
    "            continue\n",
    "\n",
    "        if type(response)==dict:\n",
    "            print(\"AN EXCEPTION: \", response)\n",
    "            time.sleep(2.5)\n",
    "            if \"Inference Endpoints\" in response['error']:\n",
    "              print(\"HOURLY RATE LIMIT REACHED\")\n",
    "              time.sleep(450)\n",
    "                \n",
    "        elif any(option in response[0]['generated_text'].split(\"'\") for option in options):\n",
    "            overloaded=0\n",
    "    response_split = response[0]['generated_text'].split(\"'\")\n",
    "    for opt in options:\n",
    "        try:\n",
    "            index = response_split.index(opt)\n",
    "        except:\n",
    "            continue\n",
    "    #print(response_split[index])\n",
    "    return response_split[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(rewards, options):\n",
    "    incorrect, correct = rewards\n",
    "\n",
    "    rule_set = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    Context: Player 1 is playing a multi-round partnership game with Player 2 for 100 rounds.\n",
    "    At each round, Player 1 and Player 2 simultaneously pick an action from the following values: {options}.\n",
    "    The payoff that both players get is determined by the following rule:\n",
    "    1. If Players play the SAME action as each other, they will both be REWARDED with payoff {correct} points.\n",
    "    2. If Players play DIFFERENT actions to each other, they will both be PUNISHED with payoff {incorrect} points. \n",
    "    The objective of each Player is to maximize their own accumulated point tally, conditional on the behavior of the other player.\n",
    "    \"\"\" \n",
    "    return rule_set\n",
    "def get_outcome(my_answer, partner_answer, rewards):\n",
    "    if my_answer == partner_answer:\n",
    "        return rewards[1]\n",
    "    return rewards[0]\n",
    "\n",
    "\n",
    "def get_prompt(player, memory_size, rules):\n",
    "\n",
    "  # add initial round\n",
    "  new_query = f\"It is now round 1.\" + \" The current score of Player 1 is 0. Answer saying which value Player 1 should pick. Please think step by step before making a decision. Remember, examining history explicitly is important. Write your answer using the following format: {'value': <VALUE_OF_PLAYER_1>; 'reason': <YOUR_REASON>}. <|eot_id|><|start_header_id|>user<|end_header_id|> Answer saying which action Player 1 should play. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "  l = len(player['my_history'])\n",
    "  if l == 0:\n",
    "    return \"\"\"\\n \"\"\".join([rules, new_query])\n",
    "  \n",
    "  current_score = 0 #local score tracking --ignores global scoring.\n",
    "  history_intro = \"This is the history of choices in past rounds:\"\n",
    "  histories = []\n",
    "  if l < memory_size:\n",
    "    for idx in range(l):\n",
    "      my_answer = player['my_history'][idx] \n",
    "      partner_answer = player['partner_history'][idx] \n",
    "      outcome = player['outcome'][idx]\n",
    "      current_score+=outcome\n",
    "      histories.append({'round':idx+1, 'Player 1':my_answer, 'Player 2':partner_answer, 'payoff':outcome})\n",
    "  \n",
    "  if l >= memory_size:\n",
    "    indices = list(range(l))[-memory_size:]\n",
    "    for idx, r in enumerate(indices):\n",
    "      my_answer = player['my_history'][r] \n",
    "      partner_answer = player['partner_history'][r] \n",
    "      outcome = player['outcome'][r] \n",
    "      current_score+=outcome\n",
    "      histories.append({'round':idx+1, 'Player 1':my_answer, 'Player 2':partner_answer, 'payoff':outcome})\n",
    "  \n",
    "  new_query = f\"It is now round {idx+2}. The current score of Player 1 is {current_score}.\" + \" Answer saying which value Player 1 should pick. Please think step by step before making a decision. Remember, examining history explicitly is important. Write your answer using the following format: {'value': <VALUE_OF_PLAYER_1>; 'reason': <YOUR_REASON>}. <|eot_id|><|start_header_id|>user<|end_header_id|> Answer saying which action Player 1 should play. <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "  histories = \"\\n \".join([f\"{hist}\" for hist in histories])\n",
    "  prompt = \"\"\"\\n \"\"\".join([rules, history_intro, histories, new_query])\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(dataframe, memory_size, rewards, options, fname, total_interactions = total_interactions):\n",
    "    new_options = options.copy()\n",
    "    player = dataframe['simulation']\n",
    "    tracker = dataframe['tracker']\n",
    "    while len(tracker['answers']) < total_interactions:\n",
    "        random.shuffle(new_options)\n",
    "        rules = get_rules(rewards, options = new_options)\n",
    "        \n",
    "        # play\n",
    "        # get prompt with rules & history of play\n",
    "        prompt = get_prompt(player, memory_size=memory_size, rules = rules)\n",
    "\n",
    "        # get agent response\n",
    "        answer = get_response(prompt, options=new_options)\n",
    "\n",
    "        tracker['answers'].append(answer)\n",
    "        \n",
    "        if len(tracker['answers']) % 20 == 0:\n",
    "            print(f\"INTERACTION {len(tracker['answers'])}\")\n",
    "            dataframe['tracker'] = tracker\n",
    "            f = open(fname, 'wb')\n",
    "            pickle.dump(dataframe, f)\n",
    "            f.close()\n",
    "\n",
    "    dataframe['tracker'] = tracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player():\n",
    "    return {'my_history': [], 'partner_history': [], 'interactions': [], 'score': 0, 'score_history': [], 'outcome': []}\n",
    "\n",
    "def update_dict(player, my_answer, partner_answer, outcome):\n",
    "  player['score'] += outcome\n",
    "#   if player['score'] < 0:\n",
    "#     player['score'] = 0 #no negative scores\n",
    "\n",
    "  player['my_history'].append(my_answer)\n",
    "  player['partner_history'].append(partner_answer)\n",
    "  player['score_history'].append(player['score'])\n",
    "  player['outcome'].append(outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observables\n",
    "P1b = [['Q', 'M'], ['M', 'Q'], ['Q', 'M'], ['M', 'Q'], ['Q', 'Q'], ['M', 'M'], ['M', 'M'], ['Q', 'Q']]\n",
    "P2b = [['M', 'Q'], ['Q', 'M'], ['M', 'M'], ['Q', 'Q'], ['Q', 'M'], ['M', 'Q'], ['M', 'M'], ['Q', 'Q']]\n",
    "P1a = [['Q'], ['Q'], ['M'], ['M']]\n",
    "P2a = [['M'], ['Q'], ['M'], ['Q']]\n",
    "def get_configuration_dataframe(fname, rewards, my_history, partner_history):\n",
    "    try:\n",
    "        return pickle.load(open(fname, 'rb'))\n",
    "    except:\n",
    "        print('DATAFILE NOT FOUND')\n",
    "    \n",
    "    dataframe = {'simulation': get_player(), 'tracker': {'answers': []}}\n",
    "    for p1, p2 in zip(my_history, partner_history):\n",
    "        outcome = get_outcome(p1, p2, rewards = rewards)\n",
    "        update_dict(dataframe['simulation'], p1, p2, outcome)\n",
    "\n",
    "    return dataframe\n",
    "P1_set = [P1a, P1b]\n",
    "P2_set = [P2a, P2b]\n",
    "for P1, P2 in zip(P1_set, P2_set):\n",
    "    for rewards in rewards_set:\n",
    "        for memory_size in memory_size_set:\n",
    "            for options in options_set:\n",
    "                for my_history, partner_history in zip(P1, P2):\n",
    "                    print(f\"my history: {my_history}, partner history: {partner_history}\")\n",
    "                    mainfname = f\"llama31_bias_test_{''.join([str(m) for m in my_history])}_{''.join([str(m) for m in partner_history])}.pkl\"\n",
    "                    print(mainfname)\n",
    "                    mainframe = get_configuration_dataframe(fname = mainfname, rewards=rewards, my_history=my_history, partner_history=partner_history)\n",
    "                    simulate(dataframe=mainframe, memory_size=memory_size, rewards=rewards, options = options, fname = mainfname, total_interactions=total_interactions)\n",
    "                    f = open(mainfname, 'wb')\n",
    "                    pickle.dump(mainframe, f)\n",
    "                    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALYSE INDIVIDUAL BIAS (BINARY CHOICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_observed_mean(count):\n",
    "    total_count = sum(count)\n",
    "    return count[1] / total_count\n",
    "\n",
    "# Function to plot the bootstrap distribution\n",
    "def plot_bootstrap_distribution(boot_means, observed_mean):\n",
    "    plt.hist(boot_means, bins=30, density=True, edgecolor='black')\n",
    "    plt.axvline(observed_mean, color='red', linestyle='--', label='Observed Mean')\n",
    "    plt.title(\"Bootstrap Distribution of Means\")\n",
    "    plt.xlabel(\"Mean\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Bootstrap function to generate bootstrap samples and calculate their means\n",
    "def bootstrap(count, num_bootstrap_samples=10000):\n",
    "    total_count = sum(count)\n",
    "    boot_means = []\n",
    "    data = np.array([0] * count[0] + [1] * count[1])  # Recreate the data from counts\n",
    "    for _ in range(num_bootstrap_samples):\n",
    "        bootstrap_sample = np.random.choice(data, size=int(0.7*total_count), replace=True)\n",
    "        boot_means.append(np.mean(bootstrap_sample))\n",
    "    return np.array(boot_means)\n",
    "\n",
    "# Function to perform a one-tailed hypothesis test\n",
    "def one_tailed_test(count, null_mean=0.5, num_bootstrap_samples=10000, alpha=0.05):\n",
    "    observed_mean = calculate_observed_mean(count)\n",
    "    boot_means = bootstrap(count, num_bootstrap_samples)\n",
    "    \n",
    "    # Determine whether it's left-tailed or right-tailed\n",
    "    if observed_mean < null_mean:\n",
    "        # Left-tailed test (bias towards 0)\n",
    "        p_value = np.sum(boot_means <= observed_mean) / len(boot_means)\n",
    "    else:\n",
    "        # Right-tailed test (bias towards 1)\n",
    "        p_value = np.sum(boot_means >= observed_mean) / len(boot_means)\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Observed Mean: {observed_mean}\")\n",
    "    print(f\"One-Tailed P-value: {p_value}\")\n",
    "    if p_value < alpha:\n",
    "        print(\"Reject the measured probability. More likely produce less extreme values\")\n",
    "    else:\n",
    "        print(\"Fail to reject the measured probability. More likely to produce more extreme values.\")\n",
    "    \n",
    "    # Plot the bootstrap distribution with observed mean\n",
    "    plot_bootstrap_distribution(boot_means, observed_mean)\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "def exact_binomial_test(counts, null_mean = 0.5):\n",
    "    observed_mean = calculate_observed_mean(counts)\n",
    "    if observed_mean > null_mean:\n",
    "        test_direction = 'greater'\n",
    "    else: \n",
    "        test_direction = 'less'\n",
    "    p_value = stats.binomtest(k= counts[1], n = sum(counts), p = 0.5, alternative=test_direction)\n",
    "    print(f\"Observed Mean: {observed_mean}\")\n",
    "    print(f\"Bias P-value: {p_value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ['Q', 'M']\n",
    "P1 = [['Q'], ['Q'], ['M'], ['M']]\n",
    "P2 = [['Q'], ['M'], ['M'], ['Q']]\n",
    "model = \"llama31\"\n",
    "for my_history, partner_history in zip(P1, P2):\n",
    "    mainfname = f\"{model}_bias_test_{''.join([str(m) for m in my_history])}_{''.join([str(m) for m in partner_history])}.pkl\"\n",
    "    dataframe = load_dataframe(fname = mainfname)\n",
    "    counts = [dataframe['tracker']['answers'].count(option) for option in options]\n",
    "    print(f\"{''.join([str(m) for m in my_history])}_{''.join([str(m) for m in partner_history])}\")\n",
    "    # test for any bias\n",
    "    print(\"probability of more extreme values than measuredif p = 0.5\")\n",
    "    exact_binomial_test(counts, null_mean=0.5)\n",
    "    # one tailed test\n",
    "    print(\"One-tailed hypothesis test:\")\n",
    "    one_tailed_test(count=counts, null_mean=0.5, num_bootstrap_samples=10000, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ['Q', 'M']\n",
    "P1 = [['Q', 'M'], ['M', 'Q'], ['Q', 'M'], ['M', 'Q'], ['Q', 'Q'], ['M', 'M'], ['M', 'M'], ['Q', 'Q']]\n",
    "P2 = [['M', 'Q'], ['Q', 'M'], ['M', 'M'], ['Q', 'Q'], ['Q', 'M'], ['M', 'Q'], ['M', 'M'], ['Q', 'Q']]\n",
    "model = \"llama31\"\n",
    "for my_history, partner_history in zip(P1, P2):\n",
    "    mainfname = f\"{model}_bias_test_{''.join([str(m) for m in my_history])}_{''.join([str(m) for m in partner_history])}\"\n",
    "    dataframe = load_dataframe(fname = mainfname)\n",
    "    counts = [dataframe['tracker']['answers'].count(option) for option in options]\n",
    "    counts = np.array(counts)\n",
    "    measured_prob = counts[1]/sum(counts)\n",
    "    print(f\"{''.join([str(m) for m in my_history])}_{''.join([str(m) for m in partner_history])}\")\n",
    "    # test for any bias\n",
    "    print(\"probability of more extreme values than measuredif p = 0.5\")\n",
    "    exact_binomial_test(counts, null_mean=0.5)\n",
    "    # one tailed test\n",
    "    print(\"One-tailed hypothesis test:\")\n",
    "    one_tailed_test(count=counts, null_mean=0.5, num_bootstrap_samples=10000, alpha=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INDIVIDUAL BIAS (W > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_test(observed, expected):\n",
    "    \"\"\"\n",
    "    Perform chi-squared test on categorical data.\n",
    "    \n",
    "    Args:\n",
    "    observed (list or np.array): List of observed frequencies\n",
    "    expected (list or np.array): List of expected frequencies\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (chi_squared_statistic, p_value)\n",
    "    \"\"\"\n",
    "    observed = np.array(observed)\n",
    "    expected = np.array(expected)\n",
    "    \n",
    "    # Calculate chi-squared statistic\n",
    "    chi_squared = np.sum((observed - expected)**2 / expected)\n",
    "    \n",
    "    # Calculate degrees of freedom\n",
    "    df = len(observed) - 1\n",
    "    \n",
    "    # Calculate p-value\n",
    "    p_value = 1 - chi2.cdf(chi_squared, df)\n",
    "    \n",
    "    return chi_squared, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ['Q', 'M', 'X', 'Y', 'F', 'J', 'P', 'R', 'C', 'D']\n",
    "fname = f\"llama31_no_memory_bias_test_{''.join([str(m) for m in options])}_0mem.pkl\"\n",
    "dataframe = load_dataframe(fname = fname)\n",
    "# count0 = dataframe['tracker']['answers'].count(params['options'][0])\n",
    "# count1 = dataframe['tracker']['answers'].count(params['options'][1])\n",
    "counts = [dataframe['tracker']['answers'].count(option) for option in options]\n",
    "answers = sum(counts)\n",
    "print(answers)\n",
    "counts = np.array(counts)\n",
    "expected = [answers/len(counts)]*len(counts)\n",
    "print(expected)\n",
    "chi_squared, p_value = chi_squared_test(observed = counts, expected = expected)\n",
    "\n",
    "print(f\"Chi-squared statistic: {chi_squared:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
