{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import yaml\n",
    "from munch import munchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    doc = yaml.safe_load(f)\n",
    "config = munchify(doc)\n",
    "# set temperature to 0 for deterministic outcomes\n",
    "temperature = config.params.temperature\n",
    "if temperature == 0:\n",
    "    llm_params = {\"do_sample\": False,\n",
    "            \"max_new_tokens\": 12,\n",
    "            \"return_full_text\": False, \n",
    "            }\n",
    "else:\n",
    "    llm_params = {\"do_sample\": True,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_k\": 10,\n",
    "            \"max_new_tokens\": 15,\n",
    "            \"return_full_text\": False, \n",
    "            }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD LANGUAGE MODEL\n",
    "API_TOKEN = ''   \n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "\n",
    "def query(payload):\n",
    "    \"Query the Hugginface API\"\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=payload).json()\n",
    "    except:\n",
    "        return None\n",
    "    return response\n",
    "\n",
    "def get_response(chat):\n",
    "    \"\"\"Generate a response from the Llama model.\"\"\"\n",
    "\n",
    "    overloaded = 1\n",
    "    while overloaded == 1:\n",
    "        response = query({\"inputs\": chat, \"parameters\": llm_params, \"options\": {\"use_cache\": False}})\n",
    "        #print(response)\n",
    "        if response == None:\n",
    "            print('CAUGHT JSON ERROR')\n",
    "            continue\n",
    "\n",
    "        if type(response)==dict:\n",
    "            print(\"AN EXCEPTION\")\n",
    "            time.sleep(2.5)\n",
    "            if \"Inference Endpoints\" in response['error']:\n",
    "              print(\"HOURLY RATE LIMIT REACHED\")\n",
    "              time.sleep(900)\n",
    "                \n",
    "        elif 'value' in response[0]['generated_text']:\n",
    "            overloaded=0\n",
    "    \n",
    "            response_split = response[0]['generated_text'].split(\";\")\n",
    "            response_split = response_split[0].split(\": \")\n",
    "            if len(response_split)<2:\n",
    "                overloaded = 1\n",
    "    print(response_split[1])\n",
    "    return response_split[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(rewards, options):\n",
    "  incorrect, correct = rewards\n",
    "  rule_set = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "  Context: Player 1 is playing a multi-round partnership game with Player 2 for 100 rounds.\n",
    "  At each round, Player 1 and Player 2 simultaneously pick an action from the following values: {options}.\n",
    "  The payoff that both players get is determined by the following rule:\n",
    "  1. If Players play the SAME action as each other, they will both be REWARDED with payoff +{correct} points.\n",
    "  2. If Players play DIFFERENT actions to each other, they will both be PUNISHED with payoff {incorrect} points.\n",
    "  The objective of each Player is to maximize their own accumulated point tally, conditional on the behavior of the other player.\n",
    "  \"\"\" \n",
    "  return rule_set\n",
    "\n",
    "def get_outcome(my_answer, partner_answer, rewards):\n",
    "    if my_answer == partner_answer:\n",
    "        return rewards[1]\n",
    "    return rewards[0]\n",
    "\n",
    "def get_question(q, i, options):\n",
    "    if q == 'min':\n",
    "        question = \"Answer saying what is the lowest payoff Player 1 can get in a single round.\"\n",
    "\n",
    "    if q == 'max':\n",
    "        question = \"Answer saying what is the highest payoff Player 1 can get in a single round.\"\n",
    "    if q == 'actions':\n",
    "        question = \"Answer saying all the action values Player 1 can pick.\"\n",
    "\n",
    "    if q == 'payoff':\n",
    "        x, y = random.choices(options, k=2)\n",
    "        question = f\"Answer saying what is Player 1's payoff in a single round if Player 1 plays {x} and Player 2 plays {y}.\"\n",
    "   \n",
    "    if q == 'round':\n",
    "        question = \"Answer saying what is the current round of the game.\"\n",
    "   \n",
    "    \n",
    "    if q == 'action_i':\n",
    "        x = random.choice([1, 2])\n",
    "        question = f\"Answer saying which action Player {x} played in round {i}.\"\n",
    "\n",
    "    if q == 'points_i':\n",
    "        question = f\"Answer saying how many points Player 1 collected in round {i}.\"\n",
    "\n",
    "    if q == 'no_actions':\n",
    "        x = random.choice(options)\n",
    "        y = random.choice([1, 2])\n",
    "        question = f\"Answer saying how many times Player {y} played action {x} overall.\"\n",
    "    \n",
    "    if q == 'no_points':\n",
    "        question = f\"Answer saying what is Player 1's current total payoff.\"\n",
    "    \n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(player, rules, question):\n",
    "    # add initial round\n",
    "    #current_score = 0 #local score tracking --ignores global scoring.\n",
    "    new_query = f\"It is now round 1.\" + \" The current score of Player 1 is 0. You are an observer who answers questions about the game using a single value. Please think step by step before making a decision. Remember, examining history explicitly is important. You write your response using the following format: {'value': <YOUR_ANSWER>; 'reason': <YOUR_REASON>}. <|eot_id|><|start_header_id|>user<|end_header_id|>\" + f\" {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    l = len(player['my_history'])\n",
    "    if l == 0:\n",
    "        return \"\"\"\\n \"\"\".join([rules, new_query])\n",
    "    \n",
    "    current_score = 0\n",
    "    history_intro = \"This is the history of choices in past rounds:\"\n",
    "    histories = []\n",
    "    for idx in range(l):\n",
    "        my_answer = player['my_history'][idx] \n",
    "        partner_answer = player['partner_history'][idx] \n",
    "        outcome = player['outcome'][idx]\n",
    "        current_score+=outcome\n",
    "        histories.append({'round':idx+1, 'Player 1':my_answer, 'Player 2':partner_answer, 'payoff':outcome})\n",
    "  \n",
    "    new_query = f\"It is now round {idx+2}. The current score of Player 1 is {current_score}.\" + \" You are an observer who answers questions about the game using a single value. Please think step by step before making a decision. Remember, examining history explicitly is important. You write your response using the following format: {'value': <YOUR_ANSWER>; 'reason': <YOUR_REASON>}. <|eot_id|><|start_header_id|>user<|end_header_id|>\" + f\" {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    histories = \"\\n \".join([f\"{hist}\" for hist in histories])\n",
    "    prompt = \"\"\"\\n \"\"\".join([rules, history_intro, histories, new_query])\n",
    "    return prompt\n",
    "\n",
    "def get_meta_prompts(some_player, memory_size, rules, options):\n",
    "    q_list = ['min', 'max', 'actions', 'payoff', 'round', 'action_i', 'points_i', 'no_actions', 'no_points']\n",
    "    if len(some_player['my_history']) == 0:\n",
    "        q_list = ['min', 'max', 'actions', 'payoff', 'round']\n",
    "        i=1\n",
    "    \n",
    "    else:\n",
    "        i = random.choice(range(len(some_player['my_history']))) + 1\n",
    "    prompts = []\n",
    "    questions = []\n",
    "    for q in q_list:\n",
    "        question = get_question(q, i, options)\n",
    "        questions.append(question)\n",
    "        prompts.append(get_prompt(some_player, rules, question = question))\n",
    "\n",
    "    return i, questions, q_list, prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_sim(q, question, running_player, i, options):\n",
    "    if q == 'min':\n",
    "        return '-50'\n",
    "    \n",
    "    if q == 'max':\n",
    "        return '100'\n",
    "    \n",
    "    if q == 'actions':\n",
    "        return f\"{options} or {list(reversed(options))}\"\n",
    "\n",
    "    if q == 'payoff':\n",
    "        for x in options:\n",
    "            if question == f\"Answer saying what is Player 1's payoff in a single round if Player 1 plays {x} and Player 2 plays {x}.\":\n",
    "                return '100'\n",
    "        return '-50'\n",
    "    \n",
    "    if q == 'round':\n",
    "        return f\"{len(running_player['my_history'])+1}\"\n",
    "\n",
    "    if q == 'action_i':\n",
    "        if question == f\"Answer saying which action Player 1 played in round {i}.\":\n",
    "            return f\"'{running_player['my_history'][i-1]}'\"\n",
    "        if question == f\"Answer saying which action Player 2 played in round {i}.\":\n",
    "            return f\"'{running_player['partner_history'][i-1]}'\"\n",
    "    \n",
    "    if q == 'points_i':\n",
    "        return str(running_player['outcome'][i-1])\n",
    "    \n",
    "    if q == 'no_points':\n",
    "        return str(sum(running_player['outcome']))\n",
    "    \n",
    "    if q == 'no_actions':\n",
    "        for x in options:\n",
    "            if question == f\"Answer saying how many times Player 1 played action {x} overall.\":\n",
    "                return str(running_player['my_history'].count(x))\n",
    "            \n",
    "            if question == f\"Answer saying how many times Player 2 played action {x} overall.\":\n",
    "                return str(running_player['partner_history'].count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(dataframe, tracker, run, p, memory_size, rewards, options, fname):\n",
    "    rules = get_rules(rewards, options = options)\n",
    "    dataframe['rules'] = rules\n",
    "    # choose random player\n",
    "    player = dataframe[run]['simulation'][p]\n",
    "    running_player = {'my_history': [], 'partner_history': [], 'outcome': []}\n",
    "    question_list = ['min', 'max', 'actions', 'payoff', 'round', 'action_i', 'points_i', 'no_actions', 'no_points']\n",
    "    temp_tracker = {q: [] for q in question_list}\n",
    "    new_options = options.copy()\n",
    "    # load their current history up to given round.\n",
    "    for t in range(len(player['my_history'])):\n",
    "        random.shuffle(new_options)\n",
    "        rules = get_rules(rewards, options = new_options)\n",
    "\n",
    "        if t < memory_size:\n",
    "            running_player['my_history'] = player['my_history'][:t]\n",
    "            running_player['partner_history'] = player['partner_history'][:t]\n",
    "            running_player['outcome'] = player['outcome'][:t]\n",
    "        else:\n",
    "            running_player['my_history'] = player['my_history'][t-memory_size:t]\n",
    "            running_player['partner_history'] = player['partner_history'][t-memory_size:t]\n",
    "            running_player['outcome'] = player['outcome'][t-memory_size:t]\n",
    "        \n",
    "        # get questions\n",
    "        i, questions, q_list, prompts = get_meta_prompts(running_player, memory_size, rules, options)\n",
    "\n",
    "        # get answers\n",
    "        responses = []\n",
    "        gold_responses = []\n",
    "        for prompt, question, q in zip(prompts, questions, q_list):\n",
    "            #print(question)\n",
    "            #print(prompt)\n",
    "            response = get_llama_response(prompt)\n",
    "            gold_response = gold_sim(q, question, running_player, i, options)\n",
    "            \n",
    "            if q == 'actions':\n",
    "                if all(option in response for option in options):\n",
    "                    temp_tracker[q].append(1)\n",
    "                    print('Success')\n",
    "                else:\n",
    "                    temp_tracker[q].append(0)\n",
    "            else:\n",
    "                print(\"GOLD: \", gold_response) \n",
    "                if gold_response in response:\n",
    "                    temp_tracker[q].append(1)\n",
    "                    print('SUCCESS')\n",
    "                else:\n",
    "                    temp_tracker[q].append(0)\n",
    "            #time.sleep(2)\n",
    "        print(f\"PLAYER {p} -- INTERACTION {t}\")\n",
    "        if t % 5 == 0:\n",
    "            tracker[p] = temp_tracker\n",
    "            f = open(fname, 'wb')\n",
    "            pickle.dump(tracker, f)\n",
    "            f.close()\n",
    "    return temp_tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUNNING THE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"llama31_70b_converged_baseline_Q_M_-50_100_5mem_complete_24ps_0.5tmp.pkl\"\n",
    "try:\n",
    "    dataframe = pickle.load(open(fname, 'rb'))\n",
    "except:\n",
    "    raise ValueError('NO DATAFILE FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [-50, +100]\n",
    "options = ['Q', 'M']\n",
    "fname = \"llama31_meta_test.pkl\"\n",
    "\n",
    "tracker = {p+1: {} for p in range(8)}\n",
    "for key in tracker.keys():\n",
    "    print(f\"STARTING PLAYER {key} META PROMPTING\")\n",
    "    tracker[key]=run(dataframe, tracker, 0, key, 5, rewards, options, fname)\n",
    "    f = open(fname, 'wb')\n",
    "    pickle.dump(tracker, f)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
